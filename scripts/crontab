# Crontab para execução periódica do scraper no Docker
# Formato: minuto hora dia mês dia-da-semana comando

# === ROTINA 0: PRIMEIRA EXECUÇÃO (Automático) ===
# Executa a cada minuto até detectar dados no banco
# Automaticamente para de executar após criar flag de conclusão
*/1 * * * * /bin/bash /app/scripts/cron_first_run_check.sh >> /proc/1/fd/1 2>&1

# === ROTINA 1: DESCOBERTA COMPLETA (Semanal) ===
# Executa toda Segunda-feira às 2:00 AM
# Descobre TODOS os bundle IDs (1-35000) e detecta mudanças
0 2 * * 1 cd /app && python3 scripts/discover_with_diff.py >> /proc/1/fd/1 2>&1

# === ROTINA 2: SCRAPING INCREMENTAL (Diário após descoberta) ===
# Executa toda Segunda-feira às 3:00 AM (1h após descoberta)
# Busca detalhes APENAS dos bundles novos detectados
0 3 * * 1 cd /app && python3 scripts/scrape_incremental.py >> /proc/1/fd/1 2>&1

# === ROTINA 3: SCRAPING COMPLETO COM AUTO-DISCOVERY (Diário) ===
# Todo dia às 6:00 AM - atualiza TODOS os bundles
# Verifica se é primeira execução e roda discovery automaticamente
0 6 * * * cd /app && python3 scripts/cron_scraper.py >> /proc/1/fd/1 2>&1

# === ROTINA 4: SCRAPING RÁPIDO COM AUTO-DISCOVERY (Meio-dia) ===
# Todo dia às 12:00 PM - segunda atualização do dia
0 12 * * * cd /app && python3 scripts/cron_scraper.py >> /proc/1/fd/1 2>&1

# === ROTINA 5: SINCRONIZAÇÃO SUPABASE (A cada 6 horas) ===
# Envia dados para Supabase (se habilitado)
0 */6 * * * cd /app && python3 -m scraper.sync_supabase >> /proc/1/fd/1 2>&1

# Nota: >> /proc/1/fd/1 redireciona output para stdout do Docker
# Isso permite ver logs com 'docker compose logs -f'

# FLUXO SEMANAL:
# Segunda 02:00 - Descobre todos os IDs + detecta novos/removidos
# Segunda 03:00 - Scraping apenas dos novos
# Diário 06:00  - Scraping completo (atualiza preços/descontos)
# Diário 12:00  - Scraping rápido (segunda atualização)
# A cada 6h     - Sync Supabase (opcional)
