"""
Script principal COMPLETO com banco de dados
Otimizado para Orange Pi com Docker - usa API oficial da Steam
"""
import asyncio
import json
import os
from pathlib import Path
from typing import List, Dict
from .scraper import BundleScraper
from .filters import BundleFilter
from .logger import Logger
from .database import Database, BundleModel, ScrapingLogModel
import datetime


async def main():
    """ExecuÃ§Ã£o completa com banco de dados e retry hÃ­brido"""
    logger = Logger('main')
    
    logger.start_operation("ExecuÃ§Ã£o do scraper com banco de dados")
    
    # Inicializa banco de dados
    db = Database()
    await db.init_db()
    logger.success("Banco de dados inicializado")
    
    # Log de execuÃ§Ã£o
    scraping_log = ScrapingLogModel(
        started_at=datetime.datetime.utcnow()
    )
    
    
    try:
        # === FASE 1: Scraping com API oficial ===
        logger.info("ðŸ“¡ Scraping via API oficial da Steam")
        
        async with BundleScraper() as scraper:
            # Busca lista de IDs
            bundle_ids = await scraper.scrape_bundle_list()
            scraping_log.bundles_found = len(bundle_ids)
            logger.info(f"Total de bundle IDs encontrados: {len(bundle_ids)}")
            
            # Processa e salva em batches (para nÃ£o sobrecarregar memÃ³ria)
            filter_service = BundleFilter()
            saved_count = 0
            all_bundles_for_stats = []
            batch_size = 100
            
            for i in range(0, len(bundle_ids), batch_size):
                batch_ids = bundle_ids[i:i + batch_size]
                batch_num = i//batch_size + 1
                
                logger.info(f"ðŸ“¦ Processando batch {batch_num} ({len(batch_ids)} bundles)...")
                
                # Scrape do batch
                batch_bundles = await scraper.scrape_bundles_batch(batch_ids)
                
                # Aplica filtros
                batch_bundles = filter_service.filter_valid(batch_bundles)
                
                # Salva no banco IMEDIATAMENTE
                logger.info(f"ðŸ’¾ Salvando batch {batch_num} no banco...")
                for bundle_data in batch_bundles:
                    try:
                        bundle_model = await db.save_bundle(bundle_data)
                        saved_count += 1
                        all_bundles_for_stats.append(bundle_data)
                    except Exception as e:
                        logger.error(f"Erro ao salvar bundle {bundle_data.get('id')}: {e}")
                        scraping_log.bundles_failed += 1
                
                logger.success(f"âœ… Batch {batch_num}: {len(batch_bundles)} bundles salvos (Total: {saved_count})")
                
                # Pequena pausa entre batches
                if i + batch_size < len(bundle_ids):
                    await asyncio.sleep(2)
            
            scraping_log.bundles_scraped = saved_count
            logger.success(f"âœ… TOTAL FINAL: {saved_count} bundles salvos no banco")
            
            # Remove duplicatas para estatÃ­sticas
            bundles = filter_service.filter_duplicates(all_bundles_for_stats)
            
            # EstatÃ­sticas
            stats = filter_service.get_statistics(bundles)
            logger.info(f"ðŸ“Š EstatÃ­sticas: {json.dumps(stats, indent=2)}")
            scraping_log.stats = stats
        
        # === AnÃ¡lise de promoÃ§Ãµes reais ===
        logger.info("\nðŸ” Analisando autenticidade dos descontos...")
        
        top_bundles = await db.get_top_discounts(limit=10)
        
        if top_bundles:
            logger.info("\n=== TOP 10 BUNDLES COM MAIOR DESCONTO ===")
            
            for i, bundle in enumerate(top_bundles, 1):
                discount_analysis = bundle.get_real_discount()
                
                emoji = "âœ…" if discount_analysis['is_real'] else "âš ï¸ "
                
                logger.info(f"\n{i}. {bundle.name}")
                logger.info(f"   Desconto: {bundle.discount}% | PreÃ§o: {bundle.currency} {bundle.final_price}")
                logger.info(f"   Jogos: {bundle.games_count} | URL: {bundle.url}")
                logger.info(f"   {emoji} {discount_analysis['reason']}")
                
                if not discount_analysis['is_real']:
                    logger.warning(
                        f"   InflaÃ§Ã£o de preÃ§o: {discount_analysis.get('inflation_percent', 0)}%"
                    )
        
        # === Export opcional para JSON (compatibilidade) ===
        logger.info("\nðŸ“¤ Exportando para JSON (backup)...")
        
        output_dir = Path(__file__).parent.parent / 'data'
        output_dir.mkdir(exist_ok=True)
        
        # Converte bundles do banco para JSON
        all_bundles_data = []
        for bundle in top_bundles:
            all_bundles_data.append({
                'id': bundle.id,
                'name': bundle.name,
                'price': {
                    'final': bundle.final_price,
                    'original': bundle.original_price,
                    'currency': bundle.currency
                },
                'discount': bundle.discount,
                'games': bundle.games,
                'games_count': bundle.games_count,
                'url': bundle.url,
                'last_updated': bundle.last_updated.isoformat() if bundle.last_updated else None
            })
        
        output_file = output_dir / 'bundles.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_bundles_data, f, indent=2, ensure_ascii=False)
        
        logger.success(f"Backup JSON salvo em: {output_file}")
        
        # Finaliza log de execuÃ§Ã£o
        scraping_log.finished_at = datetime.datetime.utcnow()
        scraping_log.success = True
        
        logger.end_operation("ExecuÃ§Ã£o do scraper", success=True)
        
        return scraping_log
    
    except Exception as e:
        logger.error(f"Erro durante execuÃ§Ã£o: {e}")
        
        scraping_log.finished_at = datetime.datetime.utcnow()
        scraping_log.success = False
        scraping_log.error_message = str(e)
        
        logger.end_operation("ExecuÃ§Ã£o do scraper", success=False)
        raise
    
    finally:
        # === FASE 3: SincronizaÃ§Ã£o com Supabase (opcional) ===
        if os.getenv('ENABLE_SUPABASE_SYNC', 'false').lower() == 'true':
            logger.info("\nâ˜ï¸  FASE 3: Sincronizando com Supabase...")
            
            try:
                from .sync_supabase import SupabaseSync
                
                sync = SupabaseSync(local_db=db)
                
                if sync.test_connection():
                    # Sincroniza bundles das Ãºltimas 24h com desconto
                    sync_stats = await sync.full_sync(
                        hours_ago=24,
                        only_with_discount=True
                    )
                    
                    logger.success(
                        f"Supabase sync: {sync_stats['success']} bundles enviados"
                    )
                else:
                    logger.warning("Supabase nÃ£o disponÃ­vel, pulando sincronizaÃ§Ã£o")
            
            except ImportError:
                logger.warning("MÃ³dulo supabase nÃ£o instalado, pulando sincronizaÃ§Ã£o")
            except Exception as e:
                logger.error(f"Erro na sincronizaÃ§Ã£o Supabase: {e}")
                logger.info("Continuando mesmo sem sync...")
        
        await db.close()


async def analyze_bundle_history(bundle_id: str):
    """Analisa histÃ³rico de um bundle especÃ­fico"""
    logger = Logger('analyzer')
    
    db = Database()
    await db.init_db()
    
    try:
        bundle = await db.get_bundle_by_id(bundle_id)
        
        if not bundle:
            logger.error(f"Bundle {bundle_id} nÃ£o encontrado no banco")
            return
        
        logger.info(f"\n=== ANÃLISE DO BUNDLE: {bundle.name} ===")
        logger.info(f"ID: {bundle.id}")
        logger.info(f"URL: {bundle.url}")
        logger.info(f"PreÃ§o atual: {bundle.currency} {bundle.final_price}")
        logger.info(f"Desconto: {bundle.discount}%")
        logger.info(f"Jogos: {bundle.games_count}")
        
        # AnÃ¡lise de desconto
        discount_analysis = bundle.get_real_discount()
        logger.info(f"\nðŸ“Š AnÃ¡lise de desconto:")
        logger.info(f"   Real? {discount_analysis['is_real']}")
        logger.info(f"   RazÃ£o: {discount_analysis['reason']}")
        
        if 'avg_regular' in discount_analysis:
            logger.info(f"   PreÃ§o regular mÃ©dio: {bundle.currency} {discount_analysis['avg_regular']}")
        
        # HistÃ³rico de preÃ§os
        if bundle.price_history:
            logger.info(f"\nðŸ“ˆ HistÃ³rico de preÃ§os ({len(bundle.price_history)} registros):")
            
            for entry in bundle.price_history[-5:]:  # Ãšltimos 5
                logger.info(
                    f"   {entry['date']}: {entry['currency']} {entry['final']} "
                    f"(desconto: {entry['discount']}%)"
                )
    
    finally:
        await db.close()


if __name__ == "__main__":
    # ExecuÃ§Ã£o principal
    asyncio.run(main())
    
    # Ou analisar bundle especÃ­fico
    # asyncio.run(analyze_bundle_history('28631'))
