"""
Script principal COMPLETO com banco de dados
Otimizado para Orange Pi com Docker - usa API oficial da Steam
"""
import asyncio
import json
import os
from pathlib import Path
from typing import List, Dict
from .scraper import BundleScraper
from .filters import BundleFilter
from .logger import Logger
from .database import Database, BundleModel, ScrapingLogModel
import datetime


async def main():
    """Execu√ß√£o completa com banco de dados e retry h√≠brido"""
    logger = Logger('main')
    
    logger.start_operation("Execu√ß√£o do scraper com banco de dados")
    
    # Inicializa banco de dados
    db = Database()
    await db.init_db()
    logger.success("Banco de dados inicializado")
    
    # Log de execu√ß√£o
    scraping_log = ScrapingLogModel(
        started_at=datetime.datetime.utcnow()
    )
    
    
    try:
        # === FASE 1: Scraping com API oficial ===
        logger.info("üì° Scraping via API oficial da Steam")
        
        async with BundleScraper() as scraper:
            # Busca lista de IDs
            bundle_ids = await scraper.scrape_bundle_list()
            scraping_log.bundles_found = len(bundle_ids)
            logger.info(f"Total de bundle IDs encontrados: {len(bundle_ids)}")
            
            # Processa e salva em batches (para n√£o sobrecarregar mem√≥ria)
            filter_service = BundleFilter()
            saved_count = 0
            all_bundles_for_stats = []
            batch_size = 100
            
            for i in range(0, len(bundle_ids), batch_size):
                batch_ids = bundle_ids[i:i + batch_size]
                batch_num = i//batch_size + 1
                
                logger.info(f"üì¶ Processando batch {batch_num} ({len(batch_ids)} bundles)...")
                
                # Scrape do batch
                batch_bundles = await scraper.scrape_bundles_batch(batch_ids)
                
                # Validar retorno
                if batch_bundles is None:
                    logger.warning(f"‚ö†Ô∏è  Batch {batch_num} retornou None, pulando...")
                    continue
                
                # Aplica filtros
                batch_bundles = filter_service.filter_valid(batch_bundles)
                
                # Salva no banco IMEDIATAMENTE
                logger.info(f"üíæ Salvando batch {batch_num} no banco...")
                for bundle_data in batch_bundles:
                    try:
                        bundle_model = await db.save_bundle(bundle_data)
                        saved_count += 1
                        all_bundles_for_stats.append(bundle_data)
                    except Exception as e:
                        logger.error(f"Erro ao salvar bundle {bundle_data.get('id')}: {e}")
                        scraping_log.bundles_failed += 1
                
                logger.success(f"‚úÖ Batch {batch_num}: {len(batch_bundles)} bundles salvos (Total: {saved_count})")
                
                # Pequena pausa entre batches
                if i + batch_size < len(bundle_ids):
                    await asyncio.sleep(2)
            
            scraping_log.bundles_scraped = saved_count
            logger.success(f"‚úÖ TOTAL FINAL: {saved_count} bundles salvos no banco")
            
            # Remove duplicatas para estat√≠sticas
            bundles = filter_service.filter_duplicates(all_bundles_for_stats)
            
            # Estat√≠sticas
            stats = filter_service.get_statistics(bundles)
            logger.info(f"üìä Estat√≠sticas: {json.dumps(stats, indent=2)}")
            scraping_log.stats = stats
        
        # === An√°lise de promo√ß√µes reais ===
        logger.info("\nüîç Analisando autenticidade dos descontos...")
        
        top_bundles = await db.get_top_discounts(limit=10)
        
        if top_bundles:
            logger.info("\n=== TOP 10 BUNDLES COM MAIOR DESCONTO ===")
            
            for i, bundle in enumerate(top_bundles, 1):
                discount_analysis = bundle.get_real_discount()
                
                emoji = "‚úÖ" if discount_analysis['is_real'] else "‚ö†Ô∏è "
                
                logger.info(f"\n{i}. {bundle.name}")
                logger.info(f"   Desconto: {bundle.discount}% | Pre√ßo: {bundle.currency} {bundle.final_price}")
                logger.info(f"   Jogos: {bundle.games_count} | URL: {bundle.url}")
                logger.info(f"   {emoji} {discount_analysis['reason']}")
                
                if not discount_analysis['is_real']:
                    logger.warning(
                        f"   Infla√ß√£o de pre√ßo: {discount_analysis.get('inflation_percent', 0)}%"
                    )
        
        # === Export opcional para JSON (compatibilidade) ===
        logger.info("\nüì§ Exportando para JSON (backup)...")
        
        output_dir = Path(__file__).parent.parent / 'data'
        output_dir.mkdir(exist_ok=True)
        
        # Converte bundles do banco para JSON
        all_bundles_data = []
        for bundle in top_bundles:
            all_bundles_data.append({
                'id': bundle.id,
                'name': bundle.name,
                'price': {
                    'final': bundle.final_price,
                    'original': bundle.original_price,
                    'currency': bundle.currency
                },
                'discount': bundle.discount,
                'games': bundle.games,
                'games_count': bundle.games_count,
                'url': bundle.url,
                'last_updated': bundle.last_updated.isoformat() if bundle.last_updated else None
            })
        
        output_file = output_dir / 'bundles.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_bundles_data, f, indent=2, ensure_ascii=False)
        
        logger.success(f"Backup JSON salvo em: {output_file}")
        
        # Finaliza log de execu√ß√£o
        scraping_log.finished_at = datetime.datetime.utcnow()
        scraping_log.success = True
        
        logger.end_operation("Execu√ß√£o do scraper", success=True)
        
        return scraping_log
    
    except Exception as e:
        logger.error(f"Erro durante execu√ß√£o: {e}")
        
        scraping_log.finished_at = datetime.datetime.utcnow()
        scraping_log.success = False
        scraping_log.error_message = str(e)
        
        logger.end_operation("Execu√ß√£o do scraper", success=False)
        raise
    
    finally:
        # === FASE 3: Sincroniza√ß√£o com Supabase (DIRECT PostgreSQL) ===
        if os.getenv('ENABLE_SUPABASE_SYNC', 'false').lower() == 'true':
            logger.info("\n‚òÅÔ∏è  FASE 3: Sincronizando com Supabase...")
            
            try:
                # Usa sync direto via PostgreSQL (mais confi√°vel que SDK)
                import subprocess
                import sys
                
                # Executa o script de sync direto
                result = subprocess.run(
                    [sys.executable, '/app/scripts/sync_supabase_direct.py'],
                    capture_output=True,
                    text=True,
                    timeout=1800  # 30 minutos timeout
                )
                
                if result.returncode == 0:
                    logger.success("‚úÖ Sync com Supabase conclu√≠do com sucesso!")
                    # Mostra √∫ltimas linhas do output
                    for line in result.stdout.strip().split('\n')[-10:]:
                        if line.strip():
                            logger.info(f"   {line}")
                else:
                    logger.error(f"‚ùå Erro no sync: {result.stderr}")
            
            except subprocess.TimeoutExpired:
                logger.error("‚ùå Sync timeout (>30min)")
            except FileNotFoundError:
                logger.warning("‚ö†Ô∏è  Script sync_supabase_direct.py n√£o encontrado")
            except Exception as e:
                logger.error(f"Erro na sincroniza√ß√£o Supabase: {e}")
                logger.info("Continuando mesmo sem sync...")
        
        await db.close()


async def analyze_bundle_history(bundle_id: str):
    """Analisa hist√≥rico de um bundle espec√≠fico"""
    logger = Logger('analyzer')
    
    db = Database()
    await db.init_db()
    
    try:
        bundle = await db.get_bundle_by_id(bundle_id)
        
        if not bundle:
            logger.error(f"Bundle {bundle_id} n√£o encontrado no banco")
            return
        
        logger.info(f"\n=== AN√ÅLISE DO BUNDLE: {bundle.name} ===")
        logger.info(f"ID: {bundle.id}")
        logger.info(f"URL: {bundle.url}")
        logger.info(f"Pre√ßo atual: {bundle.currency} {bundle.final_price}")
        logger.info(f"Desconto: {bundle.discount}%")
        logger.info(f"Jogos: {bundle.games_count}")
        
        # An√°lise de desconto
        discount_analysis = bundle.get_real_discount()
        logger.info(f"\nüìä An√°lise de desconto:")
        logger.info(f"   Real? {discount_analysis['is_real']}")
        logger.info(f"   Raz√£o: {discount_analysis['reason']}")
        
        if 'avg_regular' in discount_analysis:
            logger.info(f"   Pre√ßo regular m√©dio: {bundle.currency} {discount_analysis['avg_regular']}")
        
        # Hist√≥rico de pre√ßos
        if bundle.price_history:
            logger.info(f"\nüìà Hist√≥rico de pre√ßos ({len(bundle.price_history)} registros):")
            
            for entry in bundle.price_history[-5:]:  # √öltimos 5
                logger.info(
                    f"   {entry['date']}: {entry['currency']} {entry['final']} "
                    f"(desconto: {entry['discount']}%)"
                )
    
    finally:
        await db.close()


if __name__ == "__main__":
    # Execu√ß√£o principal
    asyncio.run(main())
    
    # Ou analisar bundle espec√≠fico
    # asyncio.run(analyze_bundle_history('28631'))
